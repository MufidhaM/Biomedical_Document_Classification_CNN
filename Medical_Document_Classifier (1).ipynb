{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('Label').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences=df2['Abstract']\n",
    "#labels=df2['Label']\n",
    "#split the data into train and test dataset\n",
    "#print(len(sentences))\n",
    "train_data=df2['Abstract']\n",
    "Y=df2['Label']\n",
    "print(len(train_data))\n",
    "bbc_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, encoding='latin-1')\n",
    "bbc_counts = bbc_vec.fit_transform(train_data.values.astype('U'))\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X = tfidf_transformer.fit_transform(bbc_counts)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#kfold = StratifiedKFold(n_splits=10, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparision of Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparision of Algorithms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn import svm\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=10)))\n",
    "models.append(('DT', tree.DecisionTreeClassifier()))\n",
    "#models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('MNB', MultinomialNB(alpha=.01)))\n",
    "models.append(('BNB', BernoulliNB(alpha=.01)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('SVM', svm.SVC()))\n",
    "models.append(('PPN', Perceptron(max_iter=50, tol=1e-3)))\n",
    "models.append(('SGD', SGDClassifier(alpha=.0001, max_iter=50,penalty=\"l1\")))\n",
    "models.append(('Ridge', RidgeClassifier(tol=1e-2, solver=\"sag\")))\n",
    "models.append(('RC', NearestCentroid()))\n",
    "models.append(('PA', PassiveAggressiveClassifier(max_iter=50, tol=1e-3)))\n",
    "models.append(('BPN', MLPClassifier()))\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "  kfold = StratifiedKFold(n_splits=10)\n",
    "  cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluationg Accuracy, precision, recall, f1-score for all algorithms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.................SVM................\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "acc=list();\n",
    "prec=list();\n",
    "recall=list();\n",
    "f1=list();\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits=5)\n",
    "model = svm.SVC()\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "  #print('TRAIN', train_index, “TEST:”, test_index);\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = Y[train_index], Y[test_index]\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred=model.predict(X_test)\n",
    "  #y_pred=FRS(X_train,y_train,X_test,0.15)\n",
    "  score=accuracy_score(y_test, y_pred)\n",
    "  acc.append(score)\n",
    "  score1=precision_score(y_test, y_pred, average='weighted')\n",
    "  prec.append(score1)\n",
    "  score2=recall_score(y_test, y_pred, average='weighted')\n",
    "  recall.append(score2)\n",
    "  score3=f1_score(y_test, y_pred, average='weighted')  \n",
    "  f1.append(score3)\n",
    "\n",
    "a = np.array(acc)\n",
    "print(\"Accuracy:\",a.mean(), a.std())\n",
    "b = np.array(prec)\n",
    "print(\"Precision:\",b.mean(), b.std())\n",
    "c = np.array(recall)\n",
    "print(\"Recall:\",c.mean(), c.std())\n",
    "d = np.array(f1)\n",
    "print(\"F1-score:\",d.mean(), d.std())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(acc)\n",
    "print(\"Accuracy:\",a.mean(), a.std())\n",
    "b = np.array(prec)\n",
    "print(\"Precision:\",b.mean(), b.std())\n",
    "c = np.array(recall)\n",
    "print(\"Recall:\",c.mean(), c.std())\n",
    "d = np.array(f1)\n",
    "print(\"F1-score:\",d.mean(), d.std())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....................SVM.....................\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "model = svm.SVC()\n",
    "#model = LogisticRegression()\n",
    "#precision_score\n",
    "scoring = ['accuracy','precision','recall','f1_weighted']\n",
    "#results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "results = cross_validate(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy=\",results[\"test_accuracy\"].mean(),results[\"test_accuracy\"].std())\n",
    "print(\"F1_score=\",results[\"test_f1_weighted\"].mean(),results[\"test_f1_weighted\"].std())\n",
    "print(\"Precision=\",results[\"test_precision\"].mean(),results[\"test_precision\"].std())\n",
    "print(\"Recall=\",results[\"test_recall\"].mean(),results[\"test_recall\"].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.................KNN................\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "acc=list();\n",
    "prec=list();\n",
    "recall=list();\n",
    "f1=list();\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits=5)\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "  #print('TRAIN', train_index, “TEST:”, test_index);\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = Y[train_index], Y[test_index]\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred=model.predict(X_test)\n",
    "  #y_pred=FRS(X_train,y_train,X_test,0.15)\n",
    "  score=accuracy_score(y_test, y_pred)\n",
    "  acc.append(score)\n",
    "  score1=precision_score(y_test, y_pred, average='weighted')\n",
    "  prec.append(score1)\n",
    "  score2=recall_score(y_test, y_pred, average='weighted')\n",
    "  recall.append(score2)\n",
    "  score3=f1_score(y_test, y_pred, average='weighted')  \n",
    "  f1.append(score3)\n",
    "\n",
    "a = np.array(acc)\n",
    "print(\"Accuracy:\",a.mean(), a.std())\n",
    "b = np.array(prec)\n",
    "print(\"Precision:\",b.mean(), b.std())\n",
    "c = np.array(recall)\n",
    "print(\"Recall:\",c.mean(), c.std())\n",
    "d = np.array(f1)\n",
    "print(\"F1-score:\",d.mean(), d.std())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...................KNN............\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "scoring = ['accuracy','precision','recall','f1_weighted']\n",
    "#results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "results = cross_validate(model, X, Y, cv=kfold, scoring=scoring)\n",
    "#print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())\n",
    "print(\"Accuracy=\",results[\"test_accuracy\"].mean(),results[\"test_accuracy\"].std())\n",
    "print(\"F1_score=\",results[\"test_f1_weighted\"].mean(),results[\"test_f1_weighted\"].std())\n",
    "print(\"Precision=\",results[\"test_precision\"].mean(),results[\"test_precision\"].std())\n",
    "print(\"Recall=\",results[\"test_recall\"].mean(),results[\"test_recall\"].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.................DT................\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "acc=list();\n",
    "prec=list();\n",
    "recall=list();\n",
    "f1=list();\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits=5)\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "  #print('TRAIN', train_index, “TEST:”, test_index);\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = Y[train_index], Y[test_index]\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred=model.predict(X_test)\n",
    "  #y_pred=FRS(X_train,y_train,X_test,0.15)\n",
    "  score=accuracy_score(y_test, y_pred)\n",
    "  acc.append(score)\n",
    "  score1=precision_score(y_test, y_pred, average='weighted')\n",
    "  prec.append(score1)\n",
    "  score2=recall_score(y_test, y_pred, average='weighted')\n",
    "  recall.append(score2)\n",
    "  score3=f1_score(y_test, y_pred, average='weighted')  \n",
    "  f1.append(score3)\n",
    "\n",
    "a = np.array(acc)\n",
    "print(\"Accuracy:\",a.mean(), a.std())\n",
    "b = np.array(prec)\n",
    "print(\"Precision:\",b.mean(), b.std())\n",
    "c = np.array(recall)\n",
    "print(\"Recall:\",c.mean(), c.std())\n",
    "d = np.array(f1)\n",
    "print(\"F1-score:\",d.mean(), d.std())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.................RF................\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "acc=list();\n",
    "prec=list();\n",
    "recall=list();\n",
    "f1=list();\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits=5)\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "  #print('TRAIN', train_index, “TEST:”, test_index);\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = Y[train_index], Y[test_index]\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred=model.predict(X_test)\n",
    "  #y_pred=FRS(X_train,y_train,X_test,0.15)\n",
    "  score=accuracy_score(y_test, y_pred)\n",
    "  acc.append(score)\n",
    "  score1=precision_score(y_test, y_pred, average='weighted')\n",
    "  prec.append(score1)\n",
    "  score2=recall_score(y_test, y_pred, average='weighted')\n",
    "  recall.append(score2)\n",
    "  score3=f1_score(y_test, y_pred, average='weighted')  \n",
    "  f1.append(score3)\n",
    "\n",
    "a = np.array(acc)\n",
    "print(\"Accuracy:\",a.mean(), a.std())\n",
    "b = np.array(prec)\n",
    "print(\"Precision:\",b.mean(), b.std())\n",
    "c = np.array(recall)\n",
    "print(\"Recall:\",c.mean(), c.std())\n",
    "d = np.array(f1)\n",
    "print(\"F1-score:\",d.mean(), d.std())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial NB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.................MNB................\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "acc=list();\n",
    "prec=list();\n",
    "recall=list();\n",
    "f1=list();\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits=5)\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "model = MultinomialNB(alpha=.01)\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "  #print('TRAIN', train_index, “TEST:”, test_index);\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = Y[train_index], Y[test_index]\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred=model.predict(X_test)\n",
    "  #y_pred=FRS(X_train,y_train,X_test,0.15)\n",
    "  score=accuracy_score(y_test, y_pred)\n",
    "  acc.append(score)\n",
    "  score1=precision_score(y_test, y_pred, average='weighted')\n",
    "  prec.append(score1)\n",
    "  score2=recall_score(y_test, y_pred, average='weighted')\n",
    "  recall.append(score2)\n",
    "  score3=f1_score(y_test, y_pred, average='weighted')  \n",
    "  f1.append(score3)\n",
    "\n",
    "a = np.array(acc)\n",
    "print(\"Accuracy:\",a.mean(), a.std())\n",
    "b = np.array(prec)\n",
    "print(\"Precision:\",b.mean(), b.std())\n",
    "c = np.array(recall)\n",
    "print(\"Recall:\",c.mean(), c.std())\n",
    "d = np.array(f1)\n",
    "print(\"F1-score:\",d.mean(), d.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BernoulliNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.................BNB................\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "acc=list();\n",
    "prec=list();\n",
    "recall=list();\n",
    "f1=list();\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits=5)\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "model = BernoulliNB(alpha=.01)\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "  #print('TRAIN', train_index, “TEST:”, test_index);\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = Y[train_index], Y[test_index]\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred=model.predict(X_test)\n",
    "  #y_pred=FRS(X_train,y_train,X_test,0.15)\n",
    "  score=accuracy_score(y_test, y_pred)\n",
    "  acc.append(score)\n",
    "  score1=precision_score(y_test, y_pred, average='weighted')\n",
    "  prec.append(score1)\n",
    "  score2=recall_score(y_test, y_pred, average='weighted')\n",
    "  recall.append(score2)\n",
    "  score3=f1_score(y_test, y_pred, average='weighted')  \n",
    "  f1.append(score3)\n",
    "\n",
    "a = np.array(acc)\n",
    "print(\"Accuracy:\",a.mean(), a.std())\n",
    "b = np.array(prec)\n",
    "print(\"Precision:\",b.mean(), b.std())\n",
    "c = np.array(recall)\n",
    "print(\"Recall:\",c.mean(), c.std())\n",
    "d = np.array(f1)\n",
    "print(\"F1-score:\",d.mean(), d.std())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#........................PPN..........\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "model = Perceptron(max_iter=50, tol=1e-3)\n",
    "scoring = ['accuracy','precision','recall','f1_weighted']\n",
    "#results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "results = cross_validate(model, X, Y, cv=kfold, scoring=scoring)\n",
    "#print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())\n",
    "print(\"Accuracy=\",results[\"test_accuracy\"].mean(),results[\"test_accuracy\"].std())\n",
    "print(\"F1_score=\",results[\"test_f1_weighted\"].mean(),results[\"test_f1_weighted\"].std())\n",
    "print(\"Precision=\",results[\"test_precision\"].mean(),results[\"test_precision\"].std())\n",
    "print(\"Recall=\",results[\"test_recall\"].mean(),results[\"test_recall\"].std())\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#........................Ridge..........\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "model = RidgeClassifier(tol=1e-2, solver=\"sag\")\n",
    "scoring = ['accuracy','precision','recall','f1_weighted']\n",
    "#results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "results = cross_validate(model, X, Y, cv=kfold, scoring=scoring)\n",
    "#print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())\n",
    "print(\"Accuracy=\",results[\"test_accuracy\"].mean(),results[\"test_accuracy\"].std())\n",
    "print(\"F1_score=\",results[\"test_f1_weighted\"].mean(),results[\"test_f1_weighted\"].std())\n",
    "print(\"Precision=\",results[\"test_precision\"].mean(),results[\"test_precision\"].std())\n",
    "print(\"Recall=\",results[\"test_recall\"].mean(),results[\"test_recall\"].std())\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PassiveAggressiveClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#........................PA..........\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "model = PassiveAggressiveClassifier(max_iter=50, tol=1e-3)\n",
    "scoring = ['accuracy','precision','recall','f1_weighted']\n",
    "#results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "results = cross_validate(model, X, Y, cv=kfold, scoring=scoring)\n",
    "#print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())\n",
    "print(\"Accuracy=\",results[\"test_accuracy\"].mean(),results[\"test_accuracy\"].std())\n",
    "print(\"F1_score=\",results[\"test_f1_weighted\"].mean(),results[\"test_f1_weighted\"].std())\n",
    "print(\"Precision=\",results[\"test_precision\"].mean(),results[\"test_precision\"].std())\n",
    "print(\"Recall=\",results[\"test_recall\"].mean(),results[\"test_recall\"].std())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_train, y_train, X_test, y_test, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(X_test)\n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural networks CNN\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=maxlen,\n",
    "                           trainable=True))\n",
    "model.add(layers.SpatialDropout1D(0.3))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(256))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(4))\n",
    "model.add(layers.Activation('softmax'))\n",
    "# model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(X_train_token, y_train_bi,\n",
    "                    batch_size=30,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train_token, y_train_bi, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_token, y_test_bi, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Model** \n",
    "\n",
    "Save the model as well as tokenizer.\n",
    "The tokenizer has to be saved because it is the vocabulary. The same tokenizer and vocabulary have to be used for accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.model.save('save/my_model.h5')\n",
    "\n",
    "# Save Tokenizer i.e. Vocabulary\n",
    "with open('save/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Live Testing**\n",
    "Loading the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models.load_model('save/my_model.h5')\n",
    " \n",
    "# load tokenizer\n",
    "tokenizer = text.Tokenizer()\n",
    "with open('save/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
